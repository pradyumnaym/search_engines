{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import urllib\n",
    "import requests\n",
    "import random\n",
    "import validators\n",
    "import threading\n",
    "import pickle\n",
    "import signal\n",
    "from requests_ip_rotator import EXTRA_REGIONS, ApiGateway\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from rocksdict import Rdict, Options\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crawler design\n",
    "\n",
    "We now have a frontier of URLs. We now need to carefully crawl all linked webpages while ensuring to only index relevant webpages. \n",
    "\n",
    "**Design of the crawler**: We can see that the crawling process is _network bound_, i.e, the bottleneck is the network latency and the server rate limit. So despite the [GIL](https://wiki.python.org/moin/GlobalInterpreterLock), we can safely use multiple threads within the same python process without any performance issues. For the purposes of small-scale crawling, we do not need multiple processes. This has some advasntages:\n",
    "\n",
    "1. We do not need complex synchronisation mechanisms between different threads. The Python GIL ensures that all shared accesses are safe. (Because threads are _concurrent_, and not _parallel_).\n",
    "2. We can simply use a python list as our shared data structure for the frontier! (List operations  such as append and pop are thread-safe by [default](https://web.archive.org/web/20201108091210/http://effbot.org/pyfaq/what-kinds-of-global-value-mutation-are-thread-safe.htm))\n",
    "\n",
    "Enqueuing and dequeueing as simply done through list append and list pop operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 61935 URLs from the frontier\n"
     ]
    }
   ],
   "source": [
    "MAX_DEPTH = 8                    # Maximum depth to crawl.\n",
    "TIME_BETWEEN_REQUESTS = 5        # Number of seconds to wait between requests to the same domain\n",
    "\n",
    "# load the frontier URLs\n",
    "\n",
    "with open('../data/frontier_urls.pkl', 'rb') as f:\n",
    "    frontier = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(frontier)} URLs from the frontier\")\n",
    "\n",
    "frontier = [(url, MAX_DEPTH) for url in frontier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_crawl_state = {\n",
    "    \"frontier\": frontier,           # list of URLs to be crawled\n",
    "    \"visited\": set(),               # list of URLs that have been crawled (we only store the URL, not the content)\n",
    "    \"failed\": set(),                   # list of URLs that have failed to be crawled.\n",
    "    \"rejected\": set(),                 # list of URLs that were rejected based on key word relevance\n",
    "    \"last_saved\": time.time()       # timestamp of the last save\n",
    "}\n",
    "\n",
    "def save_state():\n",
    "    \"\"\"\n",
    "    Save the current crawl state to disk as a pickle file.\n",
    "    \"\"\"\n",
    "    with open('../data/crawl_state.pkl', 'wb') as f:\n",
    "        pickle.dump(current_crawl_state, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing the crawl results\n",
    "\n",
    "We store the results of the crawl in a `rocksDB` instance, which is a simple key-value store. We use the `rocksdict` library that provides a nice, dict-like interface to the key-value store. This takes care of caching data on memory, and flushing the results to the database as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the dictionary file\n",
    "db = Rdict('../data/crawl_data')\n",
    "\n",
    "if os.path.exists('../data/crawl_state.pkl'):\n",
    "    with open('../data/crawl_state.pkl', 'rb') as f:\n",
    "        current_crawl_state = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crawling\n",
    "\n",
    "We use a multi-threaded crawler for the reasons discussed above. Most of the operations we use below are thread-safe except for the following: \n",
    "\n",
    "* The `pop` operation is itself atomic, but we _read_ the length of the list to pop a random URL. This can cause issues and is not thread safe. So we use a mutex lock to ensure only one thread pops at once.\n",
    "* Similar to the above, `dict` operations are thread safe, but we need to ensure only one thread reads the dict at a time. So we use another lock.\n",
    "* To ensure only one thread saves the state at a time, we use another lock.\n",
    "\n",
    "The crawling process is straightforward:\n",
    "1. Pop a URL from the frontier\n",
    "2. Check if enough time has passed since the previous request to the domain.\n",
    "3. If yes, retrieve the contents of the URL\n",
    "4. Extract contents and links from the URL.\n",
    "5. Append the links to the frontier, and save the contents of the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frontier_lock = threading.Lock()    # lock to access the frontier - needed because we read the length - not atomic!\n",
    "save_lock = threading.Lock()        # lock to save the state - we don't want to save the state multiple times at the same time\n",
    "dict_read_lock = threading.Lock()   # lock to read from the dictionary - needed because reads are not atomic\n",
    "exit_event = threading.Event()      # Event to signal an exit to all threads.\n",
    "\n",
    "# dictionary to store the last time a domain was accessed\n",
    "# we can use this to avoid hitting the same domain too frequently across different crawlers\n",
    "domain_last_accessed = {}\n",
    "\n",
    "def check_url_relevance(url_content):\n",
    "    \"\"\"check if the URL is relevant to the topic of the crawl\n",
    "    we can use a simple heuristic to check if the URL contains the keyword\n",
    "    or we can use a more sophisticated method to check the content of the page\n",
    "    to see if it is relevant\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    url : str\n",
    "        the URL to check\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the URL is relevant, False otherwise\n",
    "\n",
    "    \"\"\"\n",
    "    key_words = [\"tÃ¼bingen\", \"tuebingen\", \"boris palmer\", \"72070\", \"72072\", \"72074\", \"72076\", \"tubingen\", \"eberhard karl\"]\n",
    "\n",
    "    for keyword in key_words:\n",
    "        if keyword in url_content.lower():\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def extract_links(current_url, url_content):\n",
    "    \"\"\"extract the links from the HTML content of the URL.\n",
    "    We can use BeautifulSoup to extract the links from the HTML content\n",
    "    We need to take care of relative URLs and convert them to absolute URLs\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    current_url : str\n",
    "        the URL of the page from which the content was extracted\n",
    "    url_content : str\n",
    "        the HTML content of the URL\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        list of URLs extracted from the content\n",
    "\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(url_content, 'html.parser')\n",
    "    links = []\n",
    "    \n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        href = a_tag['href']\n",
    "        absolute_url = urllib.parse.urljoin(current_url, href)\n",
    "        links.append(absolute_url)\n",
    "    \n",
    "    return links\n",
    "\n",
    "def extract_text(url_content):\n",
    "    \"\"\"extract the text content from the HTML content of the URL.\n",
    "    We can use BeautifulSoup to extract the text from the HTML content\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    url_content : str\n",
    "        the HTML content of the URL\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        the text extracted from the content\n",
    "\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(url_content, 'html.parser')\n",
    "    return soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "def get_url_content(url):\n",
    "    \"\"\"get the content of the URL using the requests library.\n",
    "    We need to check the previous access time of the domain before we can access it\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    url : str\n",
    "        the URL to crawl\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        the content of the URL\n",
    "\n",
    "    \"\"\"\n",
    "    # We need to wait for two things: \n",
    "    # 1. We need to wait for at least 2 seconds before we crawl the same domain again\n",
    "    # 2. We need to wait for the lock to be released before we can check the domain_last_accessed\n",
    "    while True:\n",
    "\n",
    "        with dict_read_lock:\n",
    "            last_accessed = domain_last_accessed.get(urllib.parse.urlparse(url).netloc)\n",
    "\n",
    "            if last_accessed is None or time.time() - last_accessed >= TIME_BETWEEN_REQUESTS:\n",
    "                # we can access the domain. We need to update the last accessed time\n",
    "                domain_last_accessed[urllib.parse.urlparse(url).netloc] = time.time()\n",
    "                break\n",
    "        time.sleep(TIME_BETWEEN_REQUESTS - (time.time() - last_accessed))\n",
    "\n",
    "    # get the content of the URL\n",
    "    return requests.get(url, timeout=30).text\n",
    "\n",
    "def crawl_webpages():\n",
    "    \"\"\"crawl the webpages in the frontier.\n",
    "\n",
    "    This function will run indefinitely and will crawl the webpages in the frontier.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    retry_count = 0\n",
    "\n",
    "    while not exit_event.is_set():\n",
    "        \n",
    "        with frontier_lock:\n",
    "            # we pop a random URL - it is important to randomize the order of the URLs \n",
    "            # to avoid multiple crawlers hitting the same website at the same time\n",
    "\n",
    "            if len(frontier) == 0:\n",
    "                retry_count += 1\n",
    "\n",
    "                if retry_count > 10:\n",
    "                    break\n",
    "            else:\n",
    "                retry_count = 0\n",
    "                url, depth = frontier.pop(random.randrange(len(frontier)))\n",
    "\n",
    "        if url in current_crawl_state[\"visited\"] or url in current_crawl_state[\"rejected\"]:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            url_content = get_url_content(url)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to crawl {url}: {e}\")\n",
    "            current_crawl_state[\"failed\"].add((url, str(e)))\n",
    "            continue\n",
    "\n",
    "\n",
    "        # check if the URL is relevant\n",
    "        if not check_url_relevance(url_content):\n",
    "            current_crawl_state[\"rejected\"].add(url)\n",
    "            db[url] = url_content\n",
    "            continue\n",
    "\n",
    "        current_crawl_state[\"visited\"].add(url)\n",
    "        # save the text content to the dictionary\n",
    "        db[url] = extract_text(url_content)\n",
    "        # extract the links from the content\n",
    "        links = extract_links(url, url_content)\n",
    "\n",
    "        if depth > 0:\n",
    "            # add the links to the frontier\n",
    "            for link in links:\n",
    "                if link not in current_crawl_state[\"visited\"] and link not in current_crawl_state[\"failed\"]:\n",
    "                    frontier.append((link, depth-1))\n",
    "\n",
    "        # save the state every 240 seconds\n",
    "        with save_lock:\n",
    "            if time.time() - current_crawl_state[\"last_saved\"] > 240:\n",
    "                save_state()\n",
    "                current_crawl_state[\"last_saved\"] = time.time()\n",
    "\n",
    "                print(\"--------------------------------------------------\")\n",
    "                print(f\"Saved state at {time.time()}\")\n",
    "                print(f\"Visited {len(current_crawl_state['visited'])} URLs\")\n",
    "                print(f\"Frontier has {len(frontier)} URLs\")\n",
    "                print(f\"Failed to crawl {len(current_crawl_state['failed'])} URLs\")\n",
    "                print(f\"Rejected {len(current_crawl_state['rejected'])} URLs\")\n",
    "                print(\"--------------------------------------------------\")\n",
    "        \n",
    "        time.sleep(random.uniform(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to crawl http://www.vodafone-shops.de: HTTPConnectionPool(host='www.vodafone-shops.de', port=80): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x1057ccfd0>, 'Connection to www.vodafone-shops.de timed out. (connect timeout=30)'))\n",
      "Failed to crawl https://www.unimuseum.uni-tuebingen.de/de/shop: HTTPSConnectionPool(host='www.unimuseum.uni-tuebingen.de', port=443): Max retries exceeded with url: /de/shop (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)')))\n",
      "Failed to crawl javascript:linkTo_UnCryptMailto(%27ocknvq%2Cfqpcvq0vcpitgfkBwpk%5C%2Fvwgdkpigp0fg%27);: No connection adapters were found for 'javascript:linkTo_UnCryptMailto(%27ocknvq%2Cfqpcvq0vcpitgfkBwpk%5C%2Fvwgdkpigp0fg%27);'\n",
      "Failed to crawl https://www.hs-rottenburg.net/studium/msc-sence-nachhaltige-energiewirtschaft-und-technik/ansprechpersonen/studierendenverwaltung/: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /studium/msc-sence-nachhaltige-energiewirtschaft-und-technik/ansprechpersonen/studierendenverwaltung/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x105e0cfa0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hunkemoller.de/nachtwaesche/neue-nachtwaesche: HTTPSConnectionPool(host='www.hunkemoller.de', port=443): Read timed out.\n",
      "Failed to crawl https://www.hunkemoller.de/bademode/bandeau-bikini: HTTPSConnectionPool(host='www.hunkemoller.de', port=443): Max retries exceeded with url: /bademode/bandeau-bikini (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x116f93eb0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hunkemoller.de/bhs/alle-bhs/filter/100d_65d_70d_75d_80d_85d_90d?start=0&sz=24: HTTPSConnectionPool(host='www.hunkemoller.de', port=443): Max retries exceeded with url: /bhs/alle-bhs/filter/100d_65d_70d_75d_80d_85d_90d?start=0&sz=24 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x116f93d30>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://uni-tuebingen.de/einrichtungen/universitaetsbibliothek/suchen-ausleihen/information/: HTTPSConnectionPool(host='uni-tuebingen.de', port=443): Max retries exceeded with url: /einrichtungen/universitaetsbibliothek/suchen-ausleihen/information/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x116f93eb0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://policies.google.com/privacy?hl=de#footnote-server-logs: HTTPSConnectionPool(host='policies.google.com', port=443): Max retries exceeded with url: /privacy?hl=de (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x116956be0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://uni-tuebingen.de/forschung/zentren-und-institute/china-centrum-tuebingen-cct/: HTTPSConnectionPool(host='uni-tuebingen.de', port=443): Max retries exceeded with url: /forschung/zentren-und-institute/china-centrum-tuebingen-cct/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x116956c40>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hunkemoller.de/bhs/alle-bhs: HTTPSConnectionPool(host='www.hunkemoller.de', port=443): Max retries exceeded with url: /bhs/alle-bhs (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x116956b80>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/studium/msc-sence-nachhaltige-energiewirtschaft-und-technik/ansprechpersonen/studierendenverwaltung/: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /studium/msc-sence-nachhaltige-energiewirtschaft-und-technik/ansprechpersonen/studierendenverwaltung/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x116956c40>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/fileadmin/user_upload/Forschung/Forschungsprojekte/Management/WiNo/Karte-WiNo.jpg: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Read timed out.\n",
      "Failed to crawl https://uni-tuebingen.de/wirtschaft/: HTTPSConnectionPool(host='uni-tuebingen.de', port=443): Max retries exceeded with url: /wirtschaft/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x11648c3a0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/studium/ressourceneffizientes-bauen-master-studium/ansprechpersonen/studierendenverwaltung/: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /studium/ressourceneffizientes-bauen-master-studium/ansprechpersonen/studierendenverwaltung/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x11648c1c0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/informationen-fuer/alumni/verein/: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "Failed to crawl https://fit.uni-tuebingen.de/Portfolio/IndexByNode?selected=239: HTTPSConnectionPool(host='fit.uni-tuebingen.de', port=443): Max retries exceeded with url: /Portfolio/IndexByNode?selected=239 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12f7ad6a0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://uni-tuebingen.de/forschung/zentren-und-institute/european-research-center-on-contemporary-taiwan/people/visiting-scholars/prof-wu-te-mei/: HTTPSConnectionPool(host='uni-tuebingen.de', port=443): Max retries exceeded with url: /forschung/zentren-und-institute/european-research-center-on-contemporary-taiwan/people/visiting-scholars/prof-wu-te-mei/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12f7ad760>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/studium/erneuerbare-energien-bachelor-studium/ansprechpersonen/studiengangsleiterin/: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /studium/erneuerbare-energien-bachelor-studium/ansprechpersonen/studiengangsleiterin/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12f7ad6a0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hunkemoller.de/sport/grunde-sport-bh-wichtig: HTTPSConnectionPool(host='www.hunkemoller.de', port=443): Max retries exceeded with url: /sport/grunde-sport-bh-wichtig (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12f7ad760>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://epv-welt.uni-tuebingen.de/RestrictedPages/StartSearch.aspx: HTTPSConnectionPool(host='epv-welt.uni-tuebingen.de', port=443): Max retries exceeded with url: /RestrictedPages/StartSearch.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x105da97c0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hunkemoller.de/accessoires/sexy-accessoires: HTTPSConnectionPool(host='www.hunkemoller.de', port=443): Max retries exceeded with url: /accessoires/sexy-accessoires (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x105da97c0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://uni-tuebingen.de/universitaet/aktuelles-und-publikationen/newsletter-uni-tuebingen-aktuell/2024/2/index/: HTTPSConnectionPool(host='uni-tuebingen.de', port=443): Max retries exceeded with url: /universitaet/aktuelles-und-publikationen/newsletter-uni-tuebingen-aktuell/2024/2/index/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x105da97c0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/fileadmin/user_upload/Studiengaenge/SENCE/Projektarbeiten/2014-1/PVT-HFT_Stuttgart.pdf: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /fileadmin/user_upload/Studiengaenge/SENCE/Projektarbeiten/2014-1/PVT-HFT_Stuttgart.pdf (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x105da97c0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/informationen-fuer/studierende/rund-um-das-studium/busverbindung/: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /informationen-fuer/studierende/rund-um-das-studium/busverbindung/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12f296280>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hunkemoller.de/dessous/schwangerschaftsmode: HTTPSConnectionPool(host='www.hunkemoller.de', port=443): Read timed out. (read timeout=30)\n",
      "Failed to crawl https://www.abfall-kreis-tuebingen.de/entsorgen/welche-abfaelle-habe-ich/: HTTPSConnectionPool(host='www.abfall-kreis-tuebingen.de', port=443): Max retries exceeded with url: /entsorgen/welche-abfaelle-habe-ich/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12fa0a910>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://uni-tuebingen.de/fakultaeten/zentrum-fuer-islamische-theologie/zentrum/profil/: HTTPSConnectionPool(host='uni-tuebingen.de', port=443): Max retries exceeded with url: /fakultaeten/zentrum-fuer-islamische-theologie/zentrum/profil/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12fa0a970>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.youtube.com/hunkemoller: HTTPSConnectionPool(host='www.youtube.com', port=443): Max retries exceeded with url: /hunkemoller (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12fa0a8b0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hunkemoller.de/struempfe/sexy-strumpfwaren: HTTPSConnectionPool(host='www.hunkemoller.de', port=443): Max retries exceeded with url: /struempfe/sexy-strumpfwaren (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12fa0a970>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://uni-tuebingen.de/universitaet/im-dialog/kinder-uni/: HTTPSConnectionPool(host='uni-tuebingen.de', port=443): Max retries exceeded with url: /universitaet/im-dialog/kinder-uni/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12fa0a8b0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/forschung/informationen-zum-wissenschaftlichen-publizieren/: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /forschung/informationen-zum-wissenschaftlichen-publizieren/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12fa0a970>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/informationen-fuer/studierende/studium-organisieren/abschlussarbeiten/: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /informationen-fuer/studierende/studium-organisieren/abschlussarbeiten/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12fa0aa90>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hunkemoller.de/bademode/alle-bademode: HTTPSConnectionPool(host='www.hunkemoller.de', port=443): Max retries exceeded with url: /bademode/alle-bademode (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12fa0aa30>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/studium/ressourceneffizientes-bauen-master-studium/ansprechpersonen/studierendenverwaltung/: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /studium/ressourceneffizientes-bauen-master-studium/ansprechpersonen/studierendenverwaltung/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12fa0a9a0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hunkemoller.de/slips/slip-promotion3: HTTPSConnectionPool(host='www.hunkemoller.de', port=443): Max retries exceeded with url: /slips/slip-promotion3 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x1164f5160>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/informationen-fuer/studierende/rund-um-das-studium/wohnen/#navigation: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /informationen-fuer/studierende/rund-um-das-studium/wohnen/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x1164f5ee0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://uni-tuebingen.de/studium/profil/: HTTPSConnectionPool(host='uni-tuebingen.de', port=443): Max retries exceeded with url: /studium/profil/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x1164f5280>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fakultaet/: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/studium/bsc-holzwirtschaft/berufseinstieg/existenzgruendung/: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Read timed out. (read timeout=30)\n",
      "Failed to crawl https://www.hs-rottenburg.net/studium/msc-sence-nachhaltige-energiewirtschaft-und-technik/ansprechpersonen/: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /studium/msc-sence-nachhaltige-energiewirtschaft-und-technik/ansprechpersonen/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12ef88520>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://uni-tuebingen.de/universitaet/aktuelles-und-publikationen/social-media/: HTTPSConnectionPool(host='uni-tuebingen.de', port=443): Max retries exceeded with url: /universitaet/aktuelles-und-publikationen/social-media/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12ef88580>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/studium/msc-forstwirtschaft/: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/fileadmin/user_upload/Die_Hochschule/Einrichtungen_der_HFR/Informations___Medienzentrum/Rechenzentrum/Helpdesk/Teamviewer/TeamViewer_HFR.exe: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Read timed out.\n",
      "Failed to crawl https://uni-tuebingen.de/universitaet/karriere/familie-und-beruf/: HTTPSConnectionPool(host='uni-tuebingen.de', port=443): Max retries exceeded with url: /universitaet/karriere/familie-und-beruf/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x1164a9070>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://uni-tuebingen.de/universitaet/profil/geschichte-der-universitaet/: HTTPSConnectionPool(host='uni-tuebingen.de', port=443): Max retries exceeded with url: /universitaet/profil/geschichte-der-universitaet/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x1164a9190>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/informationen-fuer/studierende/rund-um-das-studium/finanzierung-/-versicherung/deutschland-stipendium/: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /informationen-fuer/studierende/rund-um-das-studium/finanzierung-/-versicherung/deutschland-stipendium/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x1164a9f40>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://uni-tuebingen.de/forschung/zentren-und-institute/european-research-center-on-contemporary-taiwan/#ut-identifier--main-content: HTTPSConnectionPool(host='uni-tuebingen.de', port=443): Max retries exceeded with url: /forschung/zentren-und-institute/european-research-center-on-contemporary-taiwan/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x1164a9a00>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/studium/bsc-erneuerbare-energien/praxis-erneuerbare-energien/auslandsaufenthalte/: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /studium/bsc-erneuerbare-energien/praxis-erneuerbare-energien/auslandsaufenthalte/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x1164a96a0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/studium/msc-ressourceneffizientes-bauen/infos-ueber-den-studiengang/was-ist-ressourceneffizientes-bauen/: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /studium/msc-ressourceneffizientes-bauen/infos-ueber-den-studiengang/was-ist-ressourceneffizientes-bauen/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x1164a9250>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/fileadmin/user_upload/Studiengaenge/SENCE/Projektarbeiten/2013-2/Casa-Sustenible-en-Isla-Isabela.pdf: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /fileadmin/user_upload/Studiengaenge/SENCE/Projektarbeiten/2013-2/Casa-Sustenible-en-Isla-Isabela.pdf (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x1164a9070>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/fileadmin/user_upload/Studiengaenge/Forstwirtschaft/Flyer/Flyer-Forstwirtschaft.pdf: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Read timed out.\n",
      "Failed to crawl https://uni-tuebingen.de/forschung/nachwuchsfoerderung/promovieren-in-tuebingen/: HTTPSConnectionPool(host='uni-tuebingen.de', port=443): Max retries exceeded with url: /forschung/nachwuchsfoerderung/promovieren-in-tuebingen/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12f1ab5b0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/studium/msc-sence-nachhaltige-energiewirtschaft-und-technik/infos-ueber-den-studiengang-sence/internationalitaet/: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /studium/msc-sence-nachhaltige-energiewirtschaft-und-technik/infos-ueber-den-studiengang-sence/internationalitaet/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12f1ab610>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hunkemoller.de/sport/sport-bhs: HTTPSConnectionPool(host='www.hunkemoller.de', port=443): Max retries exceeded with url: /sport/sport-bhs (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12f1ab5b0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/studium/msc-forstwirtschaft/: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /studium/msc-forstwirtschaft/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x11627b0d0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://uni-tuebingen.de/einrichtungen/universitaetsbibliothek/ueber-uns/adresse-anfahrt/: HTTPSConnectionPool(host='uni-tuebingen.de', port=443): Max retries exceeded with url: /einrichtungen/universitaetsbibliothek/ueber-uns/adresse-anfahrt/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x11627b9d0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hunkemoller.de/sport/sport-bh-level-1: HTTPSConnectionPool(host='www.hunkemoller.de', port=443): Max retries exceeded with url: /sport/sport-bh-level-1 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x11627b0d0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/informationen-fuer/studierende/rund-um-das-studium/studentisches-engagement/#mehr: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /informationen-fuer/studierende/rund-um-das-studium/studentisches-engagement/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x116637cd0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://uni-tuebingen.de/alumni/mitwirken/: HTTPSConnectionPool(host='uni-tuebingen.de', port=443): Max retries exceeded with url: /alumni/mitwirken/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x1166378b0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://gislab.hs-rottenburg.de/webmaps/geoportal.html: HTTPSConnectionPool(host='gislab.hs-rottenburg.de', port=443): Max retries exceeded with url: /webmaps/geoportal.html (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x116637970>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/informationen-fuer/studierende/rund-um-das-studium/studentisches-engagement/#c4405: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /informationen-fuer/studierende/rund-um-das-studium/studentisches-engagement/ (Caused by NewConnectionError(\"<urllib3.connection.HTTPSConnection object at 0x12fade6d0>: Failed to establish a new connection: [Errno 49] Can't assign requested address\"))\n",
      "Failed to crawl https://www.hs-rottenburg.net/fileadmin/user_upload/Alumni/Verzeichnis-Verarbeitungstaetigkeiten.pdf: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /fileadmin/user_upload/Alumni/Verzeichnis-Verarbeitungstaetigkeiten.pdf (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x10551de80>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/forschung/informationen-zum-wissenschaftlichen-publizieren/: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /forschung/informationen-zum-wissenschaftlichen-publizieren/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x106f4cbb0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/python@3.9/3.9.7_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to crawl https://vitruv.uni-tuebingen.de/ilias3/ilias.php?ref_id=1&cmdClass=ilrepositorygui&cmdNode=o9&baseClass=ilRepositoryGUI: HTTPSConnectionPool(host='vitruv.uni-tuebingen.de', port=443): Max retries exceeded with url: /ilias3/ilias.php?ref_id=1&cmdClass=ilrepositorygui&cmdNode=o9&baseClass=ilRepositoryGUI (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)')))\n",
      "Failed to crawl https://vitruv.uni-tuebingen.de/ilias3/ilias.php?ref_id=1&cmdClass=ilrepositorygui&cmdNode=o9&baseClass=ilRepositoryGUI: HTTPSConnectionPool(host='vitruv.uni-tuebingen.de', port=443): Max retries exceeded with url: /ilias3/ilias.php?ref_id=1&cmdClass=ilrepositorygui&cmdNode=o9&baseClass=ilRepositoryGUI (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)')))\n",
      "Failed to crawl https://www.hunkemoller.de/string-merle-rot-205296.html: HTTPSConnectionPool(host='www.hunkemoller.de', port=443): Max retries exceeded with url: /string-merle-rot-205296.html (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12f9b5c10>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/studium/bsc-holzwirtschaft/ablauf-und-inhalte/studienablauf/#mehr: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /studium/bsc-holzwirtschaft/ablauf-und-inhalte/studienablauf/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12f9b5c70>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/studium/bsc-ressourcenmanagement-wasser/infos-ueber-den-studiengang/was-ist-ressourcenmanagement-wasser/: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /studium/bsc-ressourcenmanagement-wasser/infos-ueber-den-studiengang/was-ist-ressourcenmanagement-wasser/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12f9b5d00>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/studium/msc-ressourceneffizientes-bauen/forschung-und-entwicklung/masterarbeiten-ressourceneffizientes-bauen/: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /studium/msc-ressourceneffizientes-bauen/forschung-und-entwicklung/masterarbeiten-ressourceneffizientes-bauen/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12f9b5ca0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "Failed to crawl https://www.hs-rottenburg.net/hochschule/einrichtungen-der-hfr/lehreinrichtungen/: HTTPSConnectionPool(host='www.hs-rottenburg.net', port=443): Max retries exceeded with url: /hochschule/einrichtungen-der-hfr/lehreinrichtungen/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x12f9b5d00>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mthread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# save the final state\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.7_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:1053\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1053\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.7_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:1069\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# already determined that the C code is done\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[0;32m-> 1069\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1070\u001b[0m     lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1071\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def signal_handler(sig, frame):\n",
    "    print(\"KeyboardInterrupt received, shutting down...\")\n",
    "    exit_event.set()  # Signal all threads to exit\n",
    "\n",
    "# Setup signal handling\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "# start the crawler threads\n",
    "threads = [threading.Thread(target=crawl_webpages) for _ in range(1)]\n",
    "\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "save_state()\n",
    "\n",
    "# save the final state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get('https://www.wein-bauer.de/Weine/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
