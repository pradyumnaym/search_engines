{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Crawled results for english pages.\n",
    "\n",
    "We realised that the search results should be in English. We have already crawled 100K+ webpages. So we first filter the existing data, and then resume crawling with a new frontier consisting of English URLs.\n",
    "\n",
    "We delete non-English documents from the database, and perform a surgery on the crawl state so we can continue with our new frontier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import urllib\n",
    "\n",
    "from rocksdict import Rdict, Options\n",
    "from ftlangdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-25 18:14:19--  https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 2600:9000:223e:2e00:13:6e38:acc0:93a1, 2600:9000:223e:2600:13:6e38:acc0:93a1, 2600:9000:223e:d600:13:6e38:acc0:93a1, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|2600:9000:223e:2e00:13:6e38:acc0:93a1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 131266198 (125M) [application/octet-stream]\n",
      "Saving to: ‘lid.176.bin’\n",
      "\n",
      "lid.176.bin         100%[===================>] 125.18M   301MB/s    in 0.4s    \n",
      "\n",
      "2024-06-25 18:14:20 (301 MB/s) - ‘lid.176.bin’ saved [131266198/131266198]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lang': 'en', 'score': 0.7169032692909241}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = detect(text=\"what is a bibliothek?\", low_memory=False)\n",
    "result     # Good we can have mixed docs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check frontier snippets to ensure they are in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 4715 Results\n",
      "Percentage of English snippets: 75.27%\n"
     ]
    }
   ],
   "source": [
    "# Go through the search results and detect the language of the text\n",
    "# print the final percentage of snippets that are in English\n",
    "with open('../data/search_results.pkl', 'rb') as f:\n",
    "    search_results = list(pickle.load(f))\n",
    "\n",
    "print(f\"We have {len(search_results)} Results\")\n",
    "\n",
    "english_snippets = 0\n",
    "total_snippets = 0\n",
    "\n",
    "for result in search_results:\n",
    "    for snippet in result[1]:\n",
    "        total_snippets += 1\n",
    "        language = detect(snippet['snippet'], low_memory=False)['lang']\n",
    "        if language == 'en':\n",
    "            english_snippets += 1\n",
    "print(f\"Percentage of English snippets: {english_snippets / total_snippets * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of URLs in search results:  107566\n",
      "Number of unique URLs in search results:  50311\n"
     ]
    }
   ],
   "source": [
    "# count the number of URLs in result set\n",
    "all_results = sum([x[1] for x in search_results], [])\n",
    "\n",
    "print(\"Total number of URLs in search results: \", len(all_results))\n",
    "print(\"Number of unique URLs in search results: \", len(set([x['url'] for x in all_results])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Rdict('../data/crawl_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique URLs in database:  203701\n",
      "Number of unique URLs in database with English text:  25264\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "count_english = 0\n",
    "for key, value in db.items():\n",
    "    if detect(value.replace('\\n', ' '), low_memory=False)['lang'] == 'en':\n",
    "        count_english += 1\n",
    "    count += 1\n",
    "\n",
    "print(\"Number of unique URLs in database: \", count)\n",
    "print(\"Number of unique URLs in database with English text: \", count_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rejected URLs:  178437\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'builtins.Rdict' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m db[key]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of rejected URLs: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(rejected_urls))\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of unique URLs in database after rejection: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdb\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'builtins.Rdict' has no len()"
     ]
    }
   ],
   "source": [
    "rejected_urls = []\n",
    "for key, value in db.items():\n",
    "    if detect(value.replace('\\n', ' '), low_memory=False)['lang'] != 'en':\n",
    "        rejected_urls.append(key)\n",
    "        del db[key]\n",
    "\n",
    "print(\"Number of rejected URLs: \", len(rejected_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique URLs in database:  25264\n",
      "Number of unique URLs in database with English text:  25264\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "count_english = 0\n",
    "remaining_urls = []\n",
    "for key, value in db.items():\n",
    "    if detect(value.replace('\\n', ' '), low_memory=False)['lang'] == 'en':\n",
    "        remaining_urls.append(key)\n",
    "        count_english += 1\n",
    "    count += 1\n",
    "\n",
    "print(\"Number of unique URLs in database: \", count)\n",
    "print(\"Number of unique URLs in database with English text: \", count_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of URLs in frontier:  51713\n",
      "dict_keys(['frontier', 'visited', 'failed', 'rejected', 'last_saved', 'to_visit', 'all_discovered_urls'])\n"
     ]
    }
   ],
   "source": [
    "with open('../data/crawl_state.pkl', 'rb') as f:\n",
    "    crawl_state = pickle.load(f)\n",
    "\n",
    "with open('../data/frontier_urls.pkl', 'rb') as f:\n",
    "    frontier_urls = pickle.load(f)\n",
    "\n",
    "print(\"Number of URLs in frontier: \", len(frontier_urls))\n",
    "\n",
    "crawl_state['rejected'] = set(remaining_urls).intersection(set(crawl_state['rejected']))\n",
    "crawl_state['all_discovered_urls'] = set(remaining_urls).union(crawl_state['rejected'])\n",
    "crawl_state['frontier'] = list([(x, 7, urllib.parse.urlparse(x).netloc) for x in frontier_urls])\n",
    "crawl_state['visited'] = set(remaining_urls).intersection(set(crawl_state['visited']))\n",
    "crawl_state['failed'] = set()\n",
    "crawl_state['to_visit'] = set()\n",
    "print(crawl_state.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of URLs in rejected:  18310\n",
      "Number of URLs in all_discovered_urls:  25264\n",
      "Number of URLs in frontier:  51713\n",
      "Number of URLs in visited:  6967\n",
      "Number of URLs in failed:  0\n",
      "Number of URLs in to_visit:  0\n"
     ]
    }
   ],
   "source": [
    "for key in ['rejected', 'all_discovered_urls', 'frontier', 'visited', 'failed', 'to_visit']:\n",
    "    print(f\"Number of URLs in {key}: \", len(crawl_state[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/crawl_state.pkl', 'wb') as f:\n",
    "    pickle.dump(crawl_state, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
